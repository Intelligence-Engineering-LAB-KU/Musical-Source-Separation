{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. 데이터 불러오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 참조 https://github.com/SunnerLi/SVS-UNet-PyTorch/blob/master/train.py\n",
    "import torch\n",
    "import random\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import torch.utils.data as Data\n",
    "\n",
    "class SpectrogramDataset(Data.Dataset):\n",
    "    def __init__(self, path, is_sampling):\n",
    "        self.path = path\n",
    "        self.files = sorted(os.listdir(os.path.join(path, 'mixture')))\n",
    "        self.files = [name for name in self.files if 'spec' in name]\n",
    "        self.is_sampling = is_sampling\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load the spectrogram\n",
    "        mix = np.load(os.path.join(self.path, 'mixture', self.files[idx]))\n",
    "        voc = np.load(os.path.join(self.path, 'vocals', self.files[idx]))\n",
    "\n",
    "        # Random sample\n",
    "        \n",
    "        start = random.randint(0, mix.shape[-1] - 512 - 1) if self.is_sampling else 0\n",
    "        end   = 512 if self.is_sampling else mix.shape[-1]\n",
    "        mix = mix[1:,start:start + end] #, np.newaxis]\n",
    "        voc = voc[1:,start:start + end] #, np.newaxis]\n",
    "\n",
    "\n",
    "        mix = np.asarray(mix, dtype=np.float32)\n",
    "        voc = np.asarray(voc, dtype=np.float32)\n",
    "           \n",
    "        # To tensor\n",
    "        mix = torch.from_numpy(mix) #.permute(2, 0, 1)\n",
    "        voc = torch.from_numpy(voc) #.permute(2, 0, 1)\n",
    "        return mix, voc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class BiLSTMEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self, N, E):\n",
    "        \"\"\"\n",
    "        Constructing blocks of the model based\n",
    "        on the sparse skip-filtering connections.\n",
    "        Args :\n",
    "            N      : (int) Original dimensionallity of the input.\n",
    "            E      : (int) Encoding size.\n",
    "        \"\"\"\n",
    "        super(BiLSTMEncoder, self).__init__()\n",
    "        self._N = N\n",
    "        self._E = E\n",
    "        \n",
    "        self.LSTM = nn.LSTM(input_size = self._N, num_layers=2, hidden_size=self._E, batch_first = True, dropout = 0, bidirectional = True)\n",
    "    \n",
    "    def forward (self, X):\n",
    "        # input: [B, T(unknown), N]\n",
    "        # output: [B, T, 2E(bidirectional)]\n",
    "        return self.LSTM(X)[0]\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Masker (nn.Module):\n",
    "\n",
    "    def __init__(self, E, N):\n",
    "        super(Masker, self).__init__()\n",
    "        self._N = N\n",
    "        self._E = E\n",
    "        \n",
    "        self.masker = nn.Sequential(\n",
    "            nn.Linear(2*self._E, self._E),\n",
    "            nn.ReLU(),\n",
    "            # nn.LayerNorm(),\n",
    "            nn.Linear(self._E, self._N),\n",
    "            nn.Sigmoid(),\n",
    "            # nn.LayerNorm()\n",
    "        )    \n",
    "    def forward (self, X):\n",
    "        # input: [B, T, 2E(bidirectional)]\n",
    "        # output: [B, T, N]\n",
    "        return self.masker(X)\n",
    "    \n",
    "class EncoderMasker (nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EncoderMasker, self).__init__()\n",
    "    \n",
    "    def forward(self, X, encoder, masker):\n",
    "        mask = masker(encoder(X.permute(0,2,1))).permute(0,2,1)\n",
    "        return X*mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a90d1a7c9a44cca83366db65cae3849",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=14), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display \n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "train_folder = 'data/musdb18/preprocessed/plain/valid'\n",
    "\n",
    "loader = Data.DataLoader(\n",
    "    dataset = SpectrogramDataset(train_folder, is_sampling=True),\n",
    "    batch_size=1, num_workers=0, shuffle=True\n",
    ")\n",
    "\n",
    "N = 512\n",
    "E = 512\n",
    "\n",
    "encoder = BiLSTMEncoder(N, E) #.cuda()\n",
    "masker = Masker(E,N) #.cuda()\n",
    "encoder_masker = EncoderMasker()\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "encoder_optim = optim.Adagrad(encoder.parameters())\n",
    "masker_optim = optim.Adagrad(masker.parameters())\n",
    "\n",
    "    \n",
    "num_iter = 100\n",
    "loss_trace = []\n",
    "smooth_loss=0\n",
    "print_step = int(num_iter/100.)\n",
    "\n",
    "\n",
    "encoder.train()\n",
    "masker.train()\n",
    "\n",
    "    \n",
    "for iter in range(num_iter):\n",
    "    \n",
    "    loss_sum = 0\n",
    "\n",
    "    for mix, voc in tqdm_notebook(loader):\n",
    "        \n",
    "        encoder_optim.zero_grad()\n",
    "        masker_optim.zero_grad()       \n",
    "        \n",
    "        # mix, voc = mix.cuda(), voc.cuda()\n",
    "        y_hat = encoder_masker(mix, encoder, masker)\n",
    "        loss = criterion(y_hat, voc)\n",
    "        loss.backward()\n",
    "        encoder_optim.step()\n",
    "        masker_optim.step()\n",
    "\n",
    "        loss_sum += loss.item()\n",
    "\n",
    "        \n",
    "    if( (iter+1) % print_step == 0):\n",
    "        display.clear_output(wait=True)\n",
    "        \n",
    "        if(smooth_loss == 0):\n",
    "            smooth_loss = loss_sum*1.2\n",
    "        else:\n",
    "            smooth_loss = 0.999*smooth_loss + 0.001*loss_sum\n",
    "\n",
    "        loss_trace.append(smooth_loss)\n",
    "        print('print step', print_step, ' iter' , iter)\n",
    "        plt.plot(loss_trace)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
